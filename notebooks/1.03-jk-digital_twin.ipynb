{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51aea49",
   "metadata": {},
   "source": [
    "## Let's run our creative juices!\n",
    "> *How do we improve on the agent's CoT reasoning and tool calling capabilities?*\n",
    "\n",
    "##### Trial thought process as an informed reader of \"How to win friends and influence people\"\n",
    "- Want to use principles in the book to effectively prepare myself for conversation situations, potential questions etc.\n",
    "- Query: *Harry wants {} from Dumbledore. He knows {}, design a hypothetical scenario on how he should communicate his wants with principles from the resource.*\n",
    "- Follow Introspective Agent's \"Reflection AI agentic pattern\" -> And re-design workflow. \n",
    "\n",
    "##### Implementation of simple reflective agent -> An upgrade from function calling agent\n",
    "> *Instead of running straight to stop event, pass to reflective LLM to consolidate more information needed for answer (corrective step).*\n",
    "\n",
    "> *Try this! After ToolCallEvent, create \"reflection loop\" to supplement answer with a relevant example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PHOENIX_API_KEY\"] = \"ADD YOUR PHOENIX API KEY\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"ADD YOUR PHOENIX HOSTNAME\"\n",
    "\n",
    "# If you created your Phoenix Cloud instance before June 24th, 2025,\n",
    "# you also need to set the API key as a header\n",
    "#os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ed7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinkettyee/.pyenv/versions/facinating-things/lib/python3.12/site-packages/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "DependencyConflict: requested: \"openai-agents >= 0.1.0\" but found: \"None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: facinating-things\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/fascinating-things/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=\"facinating-things\", # Default is 'default'\n",
    "  auto_instrument=True, # See 'Trace all calls made to a library' below\n",
    ")\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c244e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class StreamEvent(Event):\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput\n",
    "\n",
    "## Additional Event! \n",
    "class ReflectEvent(Event): \n",
    "    chat_history: list[ChatMessage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82f6fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class FunctionCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        ## Define judge chat engine\n",
    "        self.judge = SimpleChatEngine.from_defaults(llm=OpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> InputEvent:\n",
    "        # clear sources\n",
    "        await ctx.store.set(\"sources\", [])\n",
    "\n",
    "        # check if memory is setup\n",
    "        memory = await ctx.store.get(\"memory\", default=None)\n",
    "        if not memory:\n",
    "            memory = ChatMemoryBuffer.from_defaults(llm=self.llm)\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = memory.get()\n",
    "\n",
    "        # update context\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        # stream the response\n",
    "        response_stream = await self.llm.astream_chat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        async for response in response_stream:\n",
    "            ctx.write_event_to_stream(StreamEvent(delta=response.delta or \"\"))\n",
    "\n",
    "        # save the final response, which should have all content\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        memory.put(response.message)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        # get tool calls\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            sources = await ctx.store.get(\"sources\", default=[])\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> ReflectEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "        sources = await ctx.store.get(\"sources\", default=[])\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # update memory\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        for msg in tool_msgs:\n",
    "            memory.put(msg)\n",
    "\n",
    "        await ctx.store.set(\"sources\", sources)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        chat_history = memory.get()\n",
    "        return ReflectEvent(chat_history=chat_history)\n",
    "\n",
    "    @step ## Define 'reflection step'\n",
    "    async def reflect(\n",
    "        self, ctx: Context, ev: ReflectEvent\n",
    "    ) -> InputEvent:\n",
    "        chat_history = ev.chat_history\n",
    "        response = await self.judge.achat(\n",
    "            f\"\"\"\n",
    "            Given the response to user query with tool calls: {chat_history}, determine if a context-specific example scenario is needed. \n",
    "            If yes, return an updated response with a context-specific example scenario. \n",
    "            If no, return the original response.\n",
    "            \"\"\"\n",
    "        )\n",
    "        # format response as chat message\n",
    "        formatted_response = ChatMessage(role=\"assistant\", content=str(response))\n",
    "\n",
    "        # update memory\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        memory.put(formatted_response)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "        chat_history = memory.get()\n",
    "\n",
    "        # Return chat history to InputEvent\n",
    "        return InputEvent(input=chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b930084",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step ## Define 'reflection step'\n",
    "async def reflect(\n",
    "    self, ctx: Context, ev: ReflectEvent\n",
    ") -> InputEvent:\n",
    "    chat_history = ev.chat_history\n",
    "    response = await self.judge.achat(\n",
    "        f\"\"\"\n",
    "        Given the response to user query with tool calls: {chat_history}, determine if a context-specific example scenario is needed. \n",
    "        If yes, return an updated response with a context-specific example scenario. \n",
    "        If no, return the original response.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # format response as chat message\n",
    "    formatted_response = ChatMessage(role=\"assistant\", content=str(response))\n",
    "\n",
    "    # update memory\n",
    "    memory = await ctx.store.get(\"memory\")\n",
    "    memory.put(formatted_response)\n",
    "    await ctx.store.set(\"memory\", memory)\n",
    "    chat_history = memory.get()\n",
    "\n",
    "    # Return chat history to InputEvent\n",
    "    return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b313ff",
   "metadata": {},
   "source": [
    "#### Draw workflow with pyviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af54cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.ToolCallEvent'>\n",
      "<class 'workflows.events.StopEvent'>\n",
      "<class '__main__.ReflectEvent'>\n",
      "<class '__main__.InputEvent'>\n",
      "<class '__main__.InputEvent'>\n",
      "functionagentv2_workflow.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "draw_all_possible_flows(FunctionCallingAgent, filename=\"functionagentv2_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e74cae",
   "metadata": {},
   "source": [
    "#### Run FunctionCallingAgent workflow with Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fc02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinkettyee/.pyenv/versions/facinating-things/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.chat_engine import SimpleChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d228772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# load documents - \"../data\" folder\n",
    "docs = SimpleDirectoryReader(\"../data/books\").load_data(show_progress=True)\n",
    "\n",
    "# build vector store index - Sync & Async\n",
    "client = qdrant_client.QdrantClient(host='localhost', port=6333)\n",
    "aclient = qdrant_client.AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Set LLM & embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True)\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Initialize Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"interview_notes\",\n",
    "    client=client, \n",
    "    aclient = aclient,\n",
    "    enable_hybrid = True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm25\"\n",
    "    )\n",
    "\n",
    "# Create storage context container\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build vector store index -> Query Engine\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f9912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example response - retrieve 2 sparse, 2 dense, and filter down to 3 total hybrid results\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, \n",
    "    sparse_top_k=2, \n",
    "    hybrid_top_k=3,\n",
    "    vector_store_query_mode=\"hybrid\", \n",
    "    llm=Settings.llm, \n",
    "    # use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf47e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        name=\"Resource\",\n",
    "        description=(\"Provides information on fundamental techniques in handling people.\"\n",
    "                     \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "book_agent = FunctionCallingAgent(\n",
    "    tools=tools,\n",
    "    llm = Settings.llm,\n",
    "    timeout=120,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "681db825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry should approach the situation by expressing humility and openness. For example, if Dumbledore made a statement about the importance of following rules, Harry could respond by saying, \"Well, I thought otherwise, but I may be wrong. I frequently am. If I am mistaken, I would like to understand better. Letâ€™s examine the facts together.\" He might add, \"I remember when we discussed the importance of standing up against unjust rules, like when we talked about the Triwizard Tournament. Could we look at that in light of what you just said?\" This way, he invites a constructive dialogue without directly confronting Dumbledore, while also referencing a specific situation that illustrates his point."
     ]
    }
   ],
   "source": [
    "handler = book_agent.run(input=\"With principles from the given resource, how should Harry respond to Dumbledore if he found that the latter said something wrong?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, StreamEvent):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facinating-things",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
