{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22831e6e",
   "metadata": {},
   "source": [
    "### Enhancing LLM Observability with Llama Trace by Arize Phoenix\n",
    "https://arize.com/docs/phoenix/tracing/llm-traces\n",
    "\n",
    "*Arize Phoenix is an open-sourced LLM observability tool designed for experimentation, evaluation, and troubleshooting of AI and LLM applications. It allows AI engineers and data scientists to quickly visualize their data, evaluate performance, track down issues, and export data to improve.*\n",
    "\n",
    "*\"Phoenix uses **OpenTelemetry Protocol** to record the paths taken by requests as they propagate through multiple steps or components of an LLM application pipeline\"*\n",
    "\n",
    "*\"For example: When a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation to provide a detailed timeline of the request's execution.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff05e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PHOENIX_API_KEY\"] = \"ADD YOUR PHOENIX API KEY\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"ADD YOUR PHOENIX HOSTNAME\"\n",
    "\n",
    "# If you created your Phoenix Cloud instance before June 24th, 2025,\n",
    "# you also need to set the API key as a header\n",
    "#os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25310037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinkettyee/.pyenv/versions/facinating-things/lib/python3.12/site-packages/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "DependencyConflict: requested: \"openai-agents >= 0.1.0\" but found: \"None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: facinating-things\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/fascinating-things/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=\"facinating-things\", # Default is 'default'\n",
    "  auto_instrument=True, # See 'Trace all calls made to a library' below\n",
    ")\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e7089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial simple ReAct Agent w/o tools. \n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.agent.workflow import AgentStream\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "agent = ReActAgent(llm=llm)\n",
    "\n",
    "# Create a context to store the conversation history/session state\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a86ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I can provide an overview of \"Harry Potter and the Order of the Phoenix\" without needing to use a tool.\n",
      "Answer: \"Harry Potter and the Order of the Phoenix\" is the fifth book in J.K. Rowling's Harry Potter series. The story follows Harry Potter during his fifth year at Hogwarts School of Witchcraft and Wizardry. \n",
      "\n",
      "As the wizarding world faces the return of Lord Voldemort, the Ministry of Magic refuses to acknowledge the threat, leading to a rise in tension and fear. Harry, along with his friends Hermione Granger and Ron Weasley, forms a secret group called Dumbledore's Army to prepare students for the challenges ahead, as the new Defense Against the Dark Arts teacher, Dolores Umbridge, imposes strict rules and refuses to teach practical defense skills.\n",
      "\n",
      "Throughout the book, Harry struggles with the emotional turmoil of adolescence, the loss of loved ones, and the burden of being the Chosen One. The climax occurs during a battle at the Ministry of Magic, where Harry and his friends confront Death Eaters, leading to significant revelations and losses.\n",
      "\n",
      "The book explores themes of friendship, resistance against oppression, and the importance of standing up for what is right, even in the face of adversity."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream\n",
    "\n",
    "handler = agent.run(\"Give an overview of Harry Potter and the Order of the Phoenix.\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0aa84",
   "metadata": {},
   "source": [
    "### Define and Run Workflow for Function Agent\n",
    "- Define workflow events\n",
    "- Define workflow itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e681935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class StreamEvent(Event):\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34dd1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class FunctionCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> InputEvent:\n",
    "        # clear sources\n",
    "        await ctx.store.set(\"sources\", [])\n",
    "\n",
    "        # check if memory is setup\n",
    "        memory = await ctx.store.get(\"memory\", default=None)\n",
    "        if not memory:\n",
    "            memory = ChatMemoryBuffer.from_defaults(llm=self.llm)\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = memory.get()\n",
    "\n",
    "        # update context\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        # stream the response\n",
    "        response_stream = await self.llm.astream_chat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        async for response in response_stream:\n",
    "            ctx.write_event_to_stream(StreamEvent(delta=response.delta or \"\"))\n",
    "\n",
    "        # save the final response, which should have all content\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        memory.put(response.message)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        # get tool calls\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            sources = await ctx.store.get(\"sources\", default=[])\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "        sources = await ctx.store.get(\"sources\", default=[])\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # update memory\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        for msg in tool_msgs:\n",
    "            memory.put(msg)\n",
    "\n",
    "        await ctx.store.set(\"sources\", sources)\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        chat_history = memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ed516",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "- OpenAI models like \"GPT-4o-mini\" are function calling models which returns *response.message.additional_kwargs[\"tool_calls\"]* - a structured list of tool call instructions if the model chose to invoke tools\n",
    "- *get_tool_calls_from_response()* method reads this \"tool_calls\" array and converts each entry into *ToolSelection(...)* object, extracting metadata like tool_id, tool_name and tool_kwargs.\n",
    "- If there are no tool call instructions, it returns an empty list and hence, the loop goes straight into a StopEvent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468301a",
   "metadata": {},
   "source": [
    "#### Define query engine and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinkettyee/.pyenv/versions/facinating-things/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings\n",
    ")\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd17b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# load documents - \"../data\" folder\n",
    "docs = SimpleDirectoryReader(\"../data/books\").load_data(show_progress=True)\n",
    "\n",
    "# build vector store index - Sync & Async\n",
    "client = qdrant_client.QdrantClient(host='localhost', port=6333)\n",
    "aclient = qdrant_client.AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Set LLM & embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True)\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Initialize Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"interview_notes\",\n",
    "    client=client, \n",
    "    aclient = aclient,\n",
    "    enable_hybrid = True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm25\"\n",
    "    )\n",
    "\n",
    "# Create storage context container\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build vector store index -> Query Engine\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example response - retrieve 2 sparse, 2 dense, and filter down to 3 total hybrid results\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, \n",
    "    sparse_top_k=2, \n",
    "    hybrid_top_k=3,\n",
    "    vector_store_query_mode=\"hybrid\", \n",
    "    llm=Settings.llm, \n",
    "    # use_async=True,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"When Dumbledore is wrong, should Harry say that he is wrong? Why or why not?.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c357585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>It would be more effective for Harry to approach the situation with humility and openness rather than directly telling Dumbledore he is wrong. By expressing uncertainty and inviting a discussion, Harry can create a more constructive dialogue. This approach respects Dumbledore's intelligence and authority, reducing the likelihood of defensiveness and fostering a collaborative examination of the facts.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9195bd0",
   "metadata": {},
   "source": [
    "#### Run FunctionCalling Agent with Query Engine as tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d9ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        name=\"Resource\",\n",
    "        description=(\"Provides information on fundamental techniques in handling people.\"\n",
    "                     \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "book_agent = FunctionCallingAgent(\n",
    "    tools=tools,\n",
    "    llm = Settings.llm,\n",
    "    timeout=120,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888045ba",
   "metadata": {},
   "source": [
    "> *Stream response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5921d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry should respond to Dumbledore with humility and openness. He might say something like, \"Well, I thought otherwise, but I may be wrong. I frequently am. Letâ€™s examine the facts together.\" This approach invites constructive dialogue and fosters a positive and collaborative atmosphere without directly confronting Dumbledore."
     ]
    }
   ],
   "source": [
    "# alternative queries\n",
    "alt_query = \"Harry wants approval to go to hogsmeade from Dumbledore. He knows that Dumbledore likes Voldemort, design a hypothetical scenario on how he should communicate his wants with principles from the resource.\"\n",
    "\n",
    "handler = book_agent.run(input=\"With principles from the given resource, how should Harry respond to Dumbledore if he found that the latter said something wrong?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, StreamEvent):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c096f",
   "metadata": {},
   "source": [
    "> *From llama-trace, agent did not seem to call query engine tool to answer question.*\n",
    "\n",
    "> *Solved - Need to specify resource reference for tool call to be \"activated\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53eaf8",
   "metadata": {},
   "source": [
    "#### Try workflow FunctionAgent package directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f682e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "agent = FunctionAgent(tools=tools, llm=Settings.llm)\n",
    "\n",
    "# context to hold the session/state\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed02306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Resource with args {'input': 'Should Harry Potter tell Dumbledore if he was wrong about something?'}\n",
      "Returned: It would be more effective for Harry Potter to approach Dumbledore with humility and openness. Instead of directly stating that Dumbledore is wrong, Harry could express his thoughts by saying something like, \"I may be mistaken, but I have a different perspective on this. Can we discuss it?\" This approach fosters a collaborative dialogue and encourages a more receptive atmosphere for discussion.\n",
      "Based on the principles of effective communication, Harry Potter should approach the situation with humility and openness. Instead of directly stating that Dumbledore is wrong, he could express his thoughts in a way that invites discussion. For example, he might say, \"I may be mistaken, but I have a different perspective on this. Can we discuss it?\" \n",
      "\n",
      "This approach fosters a collaborative dialogue and creates a more receptive atmosphere for discussion, allowing for a constructive exchange of ideas rather than a confrontational one."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import ToolCallResult, AgentStream\n",
    "\n",
    "handler = agent.run(\"Given the principles from the resource: Should Harry Potter tell Dumbledore if he was wrong about something?\", ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\n",
    "            f\"Call {ev.tool_name} with args {ev.tool_kwargs}\\nReturned: {ev.tool_output}\"\n",
    "        )\n",
    "    elif isinstance(ev, AgentStream):\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891017e",
   "metadata": {},
   "source": [
    "#### Explore tracing for ReAct Agent\n",
    "\n",
    "> ***Notes:*** *ReAct Agent differs from Function Calling Agent with its loop - I.e It injects a ReAct prompt (Thought, Action, Observation loop) to LLM input in PrepEvent before emitting the InputEvent.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14112c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class StreamEvent(Event):\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b101c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\n",
    "from llama_index.core.agent.react.types import (\n",
    "    ActionReasoningStep,\n",
    "    ObservationReasoningStep,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class ReActAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: LLM | None = None,\n",
    "        tools: list[BaseTool] | None = None,\n",
    "        extra_context: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "        self.llm = llm or OpenAI()\n",
    "        self.formatter = ReActChatFormatter.from_defaults(\n",
    "            context=extra_context or \"\"\n",
    "        )\n",
    "        self.output_parser = ReActOutputParser()\n",
    "\n",
    "    @step\n",
    "    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:\n",
    "        # clear sources\n",
    "        await ctx.store.set(\"sources\", [])\n",
    "\n",
    "        # init memory if needed\n",
    "        memory = await ctx.store.get(\"memory\", default=None)\n",
    "        if not memory:\n",
    "            memory = ChatMemoryBuffer.from_defaults(llm=self.llm)\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        memory.put(user_msg)\n",
    "\n",
    "        # clear current reasoning\n",
    "        await ctx.store.set(\"current_reasoning\", [])\n",
    "\n",
    "        # set memory\n",
    "        await ctx.store.set(\"memory\", memory)\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> InputEvent:\n",
    "        # get chat history\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "        chat_history = memory.get()\n",
    "        current_reasoning = await ctx.store.get(\n",
    "            \"current_reasoning\", default=[]\n",
    "        )\n",
    "\n",
    "        # format the prompt with react instructions\n",
    "        llm_input = self.formatter.format(\n",
    "            self.tools, chat_history, current_reasoning=current_reasoning\n",
    "        )\n",
    "        return InputEvent(input=llm_input)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "        current_reasoning = await ctx.store.get(\n",
    "            \"current_reasoning\", default=[]\n",
    "        )\n",
    "        memory = await ctx.store.get(\"memory\")\n",
    "\n",
    "        response_gen = await self.llm.astream_chat(chat_history)\n",
    "        async for response in response_gen:\n",
    "            ctx.write_event_to_stream(StreamEvent(delta=response.delta or \"\"))\n",
    "\n",
    "        try:\n",
    "            reasoning_step = self.output_parser.parse(response.message.content)\n",
    "            current_reasoning.append(reasoning_step)\n",
    "\n",
    "            if reasoning_step.is_done:\n",
    "                memory.put(\n",
    "                    ChatMessage(\n",
    "                        role=\"assistant\", content=reasoning_step.response\n",
    "                    )\n",
    "                )\n",
    "                await ctx.store.set(\"memory\", memory)\n",
    "                await ctx.store.set(\"current_reasoning\", current_reasoning)\n",
    "\n",
    "                sources = await ctx.store.get(\"sources\", default=[])\n",
    "\n",
    "                return StopEvent(\n",
    "                    result={\n",
    "                        \"response\": reasoning_step.response,\n",
    "                        \"sources\": [sources],\n",
    "                        \"reasoning\": current_reasoning,\n",
    "                    }\n",
    "                )\n",
    "            elif isinstance(reasoning_step, ActionReasoningStep):\n",
    "                tool_name = reasoning_step.action\n",
    "                tool_args = reasoning_step.action_input\n",
    "                return ToolCallEvent(\n",
    "                    tool_calls=[\n",
    "                        ToolSelection(\n",
    "                            tool_id=\"fake\",\n",
    "                            tool_name=tool_name,\n",
    "                            tool_kwargs=tool_args,\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "        except Exception as e:\n",
    "            current_reasoning.append(\n",
    "                ObservationReasoningStep(\n",
    "                    observation=f\"There was an error in parsing my reasoning: {e}\"\n",
    "                )\n",
    "            )\n",
    "            await ctx.store.set(\"current_reasoning\", current_reasoning)\n",
    "\n",
    "        # if no tool calls or final response, iterate again\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> PrepEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "        current_reasoning = await ctx.store.get(\n",
    "            \"current_reasoning\", default=[]\n",
    "        )\n",
    "        sources = await ctx.store.get(\"sources\", default=[])\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            if not tool:\n",
    "                current_reasoning.append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Tool {tool_call.tool_name} does not exist\"\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                sources.append(tool_output)\n",
    "                current_reasoning.append(\n",
    "                    ObservationReasoningStep(observation=tool_output.content)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                current_reasoning.append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Error calling tool {tool.metadata.get_name()}: {e}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # save new state in context\n",
    "        await ctx.store.set(\"sources\", sources)\n",
    "        await ctx.store.set(\"current_reasoning\", current_reasoning)\n",
    "\n",
    "        # prep the next iteraiton\n",
    "        return PrepEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0144cd8",
   "metadata": {},
   "source": [
    "#### Run ReAct Agent with Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e942b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: Resource\n",
      "Action Input: {\"input\": \"How should Harry respond to Dumbledore if he found that the latter said something wrong?\"}Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Harry should respond to Dumbledore with humility and openness. He could say something like, \"I thought differently, but I may be wrong. I often am. Let's examine the facts together.\" This approach fosters a collaborative discussion rather than confrontation, allowing for a more constructive dialogue."
     ]
    }
   ],
   "source": [
    "# Define agent\n",
    "react_agent = ReActAgent(\n",
    "    tools=tools,\n",
    "    llm = Settings.llm,\n",
    "    timeout=120,\n",
    "    verbose = False,\n",
    ")\n",
    "\n",
    "handler = react_agent.run(input=\"With principles from the given resource, how should Harry respond to Dumbledore if he found that the latter said something wrong?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, StreamEvent):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facinating-things",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
