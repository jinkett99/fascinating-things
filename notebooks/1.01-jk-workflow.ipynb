{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd26eba",
   "metadata": {},
   "source": [
    "## Building-up our Multi-Agent LLM System with LlamaIndex Workflow\n",
    "*Creating a digital twin of a Data Scientist Job Candidate for Interview Preparations.*\n",
    "\n",
    "> *Here are the defined/outlined steps:*\n",
    "- Load personal notes as documents for RAG system\n",
    "- Building a Hybrid Retrieval Query Engine with Qdrant. \n",
    "- Building out websearch tool\n",
    "- Integrating agents, events and tools into LlamaIndex workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c404c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tavily'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleChatEngine\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReActAgent, FunctionAgent\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtavily\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncTavilyClient\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tavily'"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.chat_engine import SimpleChatEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fb21b",
   "metadata": {},
   "source": [
    "> *Set up Qdrant vector database with \"docker run ...\" command*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9297b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:  11%|█         | 1/9 [00:00<00:03,  2.54it/s]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Loading files: 100%|██████████| 9/9 [00:00<00:00, 11.55it/s]\n",
      "Fetching 18 files: 100%|██████████| 18/18 [00:01<00:00, 15.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# load documents - \"../data\" folder\n",
    "docs = SimpleDirectoryReader(\"../data/notes\").load_data(show_progress=True)\n",
    "\n",
    "# build vector store index - Sync & Async\n",
    "client = qdrant_client.QdrantClient(host='localhost', port=6333)\n",
    "aclient = qdrant_client.AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Set LLM & embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True)\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Initialize Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"interview_notes\",\n",
    "    client=client, \n",
    "    aclient = aclient,\n",
    "    enable_hybrid = True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm25\"\n",
    "    )\n",
    "\n",
    "# Create storage context container\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build vector store index -> Query Engine\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e6208ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example response - retrieve 2 sparse, 2 dense, and filter down to 3 total hybrid results\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, \n",
    "    sparse_top_k=2, \n",
    "    hybrid_top_k=3,\n",
    "    vector_store_query_mode=\"hybrid\", \n",
    "    llm=Settings.llm, \n",
    "    # use_async=True,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Write a self introduction for an interview for Data Scientist position with Singtel.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e32f630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Hello, my name is [Your Name], and I am excited about the opportunity to interview for the Data Scientist position with Singtel. I have a strong background in data science and business statistics, having worked with the Business Statistics Division at SingStat. In my role, I was involved in various projects, including the Statistical Business Register and Business Data Analytics teams, where I developed and maintained data extraction functions and collaborated with external agencies to support policy formulation.\n",
       "\n",
       "My passion lies in leveraging data to derive actionable insights, particularly in the areas of natural language processing and machine learning. I have experience in building and refining machine learning classifiers, running statistical analyses, and translating complex data findings into clear, actionable insights for stakeholders. I believe that my skills in data analysis and my collaborative approach would make me a valuable addition to the Singtel team.\n",
       "\n",
       "Outside of my professional experience, I am committed to continuous learning and self-development, which I believe is essential in the ever-evolving field of data science. I am also an avid sports enthusiast and enjoy reading, which helps me maintain a balanced perspective.\n",
       "\n",
       "Thank you for considering my application. I look forward to discussing how my background and skills align with the goals of Singtel.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c3cb",
   "metadata": {},
   "source": [
    "> *Consider using structured outputs for workflow.*\n",
    "\n",
    "> *Idea is to retrieve chunks from \"past experiences\" as a memory on to personalise messages and answers, like a digital twin.*\n",
    "\n",
    "> *Structure outputs to complement web-search agent - Eg. Related work experiences, how current pursuits fit with the role etc.*\n",
    "\n",
    "> *Index contains information pertaining to interview preparation notes: research, background, questions and answers, personal questions and self-reflections after interviews.*\n",
    "\n",
    "**Outputs from Query Engine:**\n",
    "1. Relevant research follow-up (if any): str\n",
    "2. Relevant background information (if any): str\n",
    "3. Relevant Q&A (if any): str\n",
    "\n",
    "> *Use another agent to formulate questions for research agent.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf711fc",
   "metadata": {},
   "source": [
    "#### Thought process in interview preparation\n",
    "1. Look at Job Description - Upload to system. \n",
    "2. Research and find out more about the team and what they do - formulate agent utilising JD to formulate questions + web-search agent to search the web.\n",
    "3. Structure response for (1) work experience and (2) Why {role}? - RAG agent with context. \n",
    "4. Prepare and structure responses for General Questions - RAG agent with context (can include loop back to check for past question bank)\n",
    "5. Prepare and structure responses for technical questions - Web-search agent\n",
    "6. Prepare questions for team - Compile agent. \n",
    "7. Review and structure outputs - Compile/Review agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee2682",
   "metadata": {},
   "source": [
    "#### PDF extraction - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00bc072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Job Title: Data Scientist – Decision Science (DSAI)  \n",
      " \n",
      " \n",
      " \n",
      "HTX is the world’s first Science and Technology agency that integrates a diverse range of \n",
      "scientific and engineering capabilities to innovate and deliver transformative and operationally -\n",
      "ready solutions for homeland security. As a statutory board of the Ministry of Home Affairs \n",
      "(MHA) and integral to the Home Team, HTX works at the forefront of science and technology to \n",
      "empower Singapore’s frontline of security. Our shared mission is to amplify, augment and \n",
      "accelerate the Home Team’s advantage and secure Singapore as the safest place on planet earth.  \n",
      "Join us if you would like to be part of HTX’s force multiplier to exponentially impact \n",
      "Singapore’s safety and security.  \n",
      " \n",
      " \n",
      "We are looking for someone who is creative, curious, collaborative and enjoys challenges.  If \n",
      "you like to design, build and deploy scalable optimisation/simulation solutions, we want you!  \n",
      " \n",
      "Successful candidate’s primary job role would be in operations research, there will also be \n",
      "opportunity for job holder to gain exposure to other related work. We believe in upskilling our \n",
      "officers and exposing them to work spanning from end -to-end of the optimisation/simulation \n",
      "lifecycle.  \n",
      "Job Scope / Key deliverables:  \n",
      " • Work in a Data Science team with other stakeholders to identify, scope, \n",
      "develop and implement data science and AI solutions to address users’ needs.  \n",
      " • Apply Data Science (machine learning and deep learning) and Operations \n",
      "Research techniques ( simulation and mathematical programming  such as linear \n",
      "programming, mixed integer programming) to address the diverse business needs for \n",
      "MHA  \n",
      " • Conceptualize and design the digital twin solutions according to user \n",
      "specifications  \n",
      " • Collaborate closely with business users to extract actionable insights from \n",
      "data and processes and develop data -driven & optimised solutions to meet their needs  \n",
      " • Present analytical insights to business users and stakeholders  \n",
      " • Manage large scale and complex projects which may be developed in -\n",
      "house or co -developed with external or consultancy agencies and/or academic institutions.  \n",
      " • Strong understanding and actual project experience in one or more of the \n",
      "following areas preferred:  \n",
      " • Agent -based simulation / Traffic simulation / Discrete -event simulation / \n",
      "Physics simulation  \n",
      " • Machine learning and deep learning  \n",
      " • Artificial intelligence algorithms  \n",
      " • Data mining and knowledge discovery  \n",
      " • Statistical forecasting methods  \n",
      " \n",
      " \n",
      "Job Requirements:  \n",
      " • Candidates should possess at least a Bachelor degree in a quantitative \n",
      "discipline: Data Science, O perations Research , Engineering, Mathematics, Statistics or \n",
      "related disciplines.  \n",
      " • Candidates must be good problem solvers with strong analytical and \n",
      "mathematical skills. Background in data science or operations research, particularly \n",
      "knowledge in modelling and simulation, is required. Candidates preferably have at least 2 \n",
      "years of relevant working experience.  \n",
      " • Experience in data wrangling (merging data and data transformation) or \n",
      "user interface development and proficiency in programming languages such as Python, R, \n",
      "Java and C/C++/C# is an advantage.  \n",
      " • Experience or interest in modelling and simulation with tools such as \n",
      "AnyLogic, Vissim, ExtendSim.  \n",
      " • Experience or interest in physics simulation with Unity or other 3D \n",
      "simulation platforms.  \n",
      " • Able to work well both independently and as part of a team.  \n",
      " • Able to communicate technical aspects effectively to non -technical \n",
      "audience.  \n",
      " \n",
      " \n",
      "All new appointees will be appointed on a two -year contract in the first instance.  \n",
      "As part of the shortlisting process for this role, you may be required to complete a medical \n",
      "declaration and/or undergo further assessment. Please note that you may also be required to \n",
      "declare your vaccination status.  \n",
      "All applicants will be updated on the status of their applications within 4 weeks upon closing of \n",
      "the advertisement.  \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../pdf/JD Data Scientist - PM&S.pdf\")\n",
    "all_text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3e4f7",
   "metadata": {},
   "source": [
    "#### Define simple chat engine to return a list of questions for company websearch\n",
    "- Manager LLM corresponds to \"my thought process\", like a \"mini-me\"/digital twin chain of thought reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat engine. \n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True)\n",
    "manager_llm = SimpleChatEngine.from_defaults(llm=llm)\n",
    "\n",
    "# Get response with prompt\n",
    "response = manager_llm.chat(\n",
    "    f\"\"\"\n",
    "    Given the job description: {all_text}, \n",
    "    Formulate questions to: \n",
    "    1. Clarify job scope/key deliverables (if neccessary).\n",
    "    2. Find out more about the team and specific projects taken by the team.\n",
    "    Return only a concise list of questions, each as an independent google search query for web search.\n",
    "    Limit response to 5 questions.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99080efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What are the specific responsibilities of a Data Scientist in the Decision Science team at HTX?\n",
      "2. Can you provide examples of recent projects undertaken by the Data Science team at HTX?\n",
      "3. What types of data science and AI solutions have been implemented by HTX to address user needs?\n",
      "4. How does HTX define success for the projects managed by the Data Science team?\n",
      "5. What tools and technologies are primarily used by the Data Science team at HTX?\n",
      "6. How does the collaboration process work between the Data Science team and business users at HTX?\n",
      "7. What opportunities for professional development and upskilling are available for Data Scientists at HTX?\n",
      "8. What is the typical project lifecycle for data science initiatives at HTX?\n",
      "9. How does HTX measure the impact of its data-driven solutions on homeland security?\n",
      "10. What are the key challenges faced by the Data Science team at HTX in their projects?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b413d",
   "metadata": {},
   "source": [
    "#### Simple workflow to coordinate question formulation and web-search loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ff904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies for workflow and setup events\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.agent.workflow import ReActAgent, FunctionAgent\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Dict, Optional, Any, Union, Annotated\n",
    "from tavily import AsyncTavilyClient\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43797d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agents\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=\"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\")\n",
    "    return str(await client.search(query))\n",
    "\n",
    "web_agent = FunctionAgent(\n",
    "    tools=[search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")\n",
    "\n",
    "# Define chat engine - \"My thought process\"\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, max_tokens=1024, streaming=True)\n",
    "manager_llm = SimpleChatEngine.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84086bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define events\n",
    "class structured_jd(BaseModel): \n",
    "    company_description: str = Field(..., description=\"Brief overview about company and team in point form with newline.\")\n",
    "    job_scope: str = Field(..., description=\"Concise summary of job scope and key deliverables in point form with newline.\")\n",
    "    example_projects: str = Field(..., description=\"Given context of job description, summarize projects (if any). If no projects detailed, return a string named hypothetical_project.\")\n",
    "    questions_ind: Literal[True, False] = Field(..., description=\"Deduce if there is a need to search the web for more information (If no example projects found, return True) If yes, return True, else False.\")\n",
    "    questions: Optional[str] = Field(None, description=\"If questions_ind=True, return questions used for web-search. Ensure company name and job role is in each individual questions for search specificity.\")\n",
    "\n",
    "class WebSearchEvent(Event): \n",
    "    query: str\n",
    "\n",
    "class CompileEvent(Event): \n",
    "    company_description: str\n",
    "    job_scope: str\n",
    "    example_projects: str\n",
    "    web_responses: Optional[str]\n",
    "    source: str\n",
    "\n",
    "# Define workflow\n",
    "class simpleFlow(Workflow): \n",
    "    def __init__(self, manager_llm=llm, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.web_agent = web_agent\n",
    "        self.llm=llm\n",
    "        self.manager_llm = SimpleChatEngine.from_defaults(llm=manager_llm)\n",
    "        self.summary_prompt_template = PromptTemplate(\n",
    "            \"\"\"Given the job description pdf: {input}, return a structured output of the given fields.\"\"\"\n",
    "        )\n",
    "\n",
    "    @step \n",
    "    # Modify: To generate structured response from JD, then evaluate if supplementation needed. If yes, redirect to WebSearchEvent\n",
    "    async def internalise_jd(self, ctx: Context, ev: StartEvent) -> WebSearchEvent | CompileEvent:\n",
    "        pdf_input = ev.input\n",
    "        # Get response with prompt\n",
    "        response =  await self.llm.astructured_predict(\n",
    "            structured_jd,\n",
    "            self.summary_prompt_template,\n",
    "            input = pdf_input\n",
    "        )\n",
    "        \n",
    "        # if there are questions, go to WebSearchEvent\n",
    "        if response.questions_ind: \n",
    "            await ctx.set(\"structured_jd\", response)\n",
    "            return WebSearchEvent(query=str(response.questions))\n",
    "        # else, go to compile event\n",
    "        else: \n",
    "            return CompileEvent(company_description=response.company_description, job_scope=response.job_scope, example_projects=response.example_project, source=\"manager_llm\")\n",
    "    \n",
    "    @step\n",
    "    async def web_search(self, ctx: Context, ev: WebSearchEvent) -> CompileEvent:\n",
    "        qns = ev.query\n",
    "        response = await ctx.get(\"structured_jd\")\n",
    "        web_response = await web_agent.run(user_msg=f\"Search and return questions and answers with /n. Questions: {qns}\")\n",
    "        return CompileEvent(company_description=response.company_description, job_scope=response.job_scope, example_projects=response.example_projects, web_responses=str(web_response), source=\"both\")\n",
    "    \n",
    "    @step\n",
    "    async def compile(self, ctx: Context, ev: CompileEvent) -> StopEvent: # Return StopEvent to end loop first.\n",
    "        response = await self.manager_llm.achat(\n",
    "            f\"\"\"\n",
    "            Internalise company description {ev.company_description} and job scope {ev.job_scope}.\n",
    "            If there are no example projects {ev.example_projects}, supplement it with web-search results {ev.web_responses}. \n",
    "            Summarize all the above information into a neat write-up with 3 sections: Company description, job scope/key deliverables and example projects (with their respective weblinks).\n",
    "            The report contents should be grounded in the information provided only. Return in neat, markdown format.\n",
    "            \"\"\"\n",
    "        )\n",
    "        return StopEvent(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a57463",
   "metadata": {},
   "source": [
    "#### Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc0468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinkettyee/.pyenv/versions/facinating-things/lib/python3.12/site-packages/PyPDF2/generic/_data_structures.py:1072: RuntimeWarning: coroutine 'Context.get' was never awaited\n",
      "  peek = stream.read(20)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/var/folders/nr/6b6zx3jn687ghmtz2_2dw_b40000gn/T/ipykernel_29687/1484249634.py:43: DeprecationWarning: Context.set(key, value) is deprecated. Use 'await ctx.store.set(key, value)' instead.\n",
      "  await ctx.set(\"structured_jd\", response)\n",
      "/var/folders/nr/6b6zx3jn687ghmtz2_2dw_b40000gn/T/ipykernel_29687/1484249634.py:52: DeprecationWarning: Context.get() is deprecated. Use 'await ctx.store.get()' instead.\n",
      "  response = await ctx.get(\"structured_jd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HTX Overview\n",
      "\n",
      "## Company Description\n",
      "HTX is the world’s first Science and Technology agency dedicated to integrating a diverse range of scientific and engineering capabilities to innovate and deliver transformative and operationally-ready solutions for homeland security. As a statutory board under the Ministry of Home Affairs (MHA), HTX operates at the forefront of science and technology to empower Singapore’s frontline security efforts. Their mission is to amplify, augment, and accelerate the Home Team’s advantage, ensuring Singapore remains the safest place on earth.\n",
      "\n",
      "## Job Scope / Key Deliverables\n",
      "The Data Science team at HTX is responsible for identifying, scoping, developing, and implementing data science and AI solutions tailored to meet the business needs of the MHA. Key responsibilities include:\n",
      "- Applying Data Science and Operations Research techniques to address various business challenges.\n",
      "- Conceptualizing and designing digital twin solutions based on user specifications.\n",
      "- Collaborating with business users to extract actionable insights and develop optimized solutions.\n",
      "- Presenting analytical insights to business users and stakeholders.\n",
      "- Managing large-scale and complex projects, whether developed in-house or in collaboration with external agencies.\n",
      "\n",
      "## Example Projects\n",
      "HTX has undertaken various projects that leverage data science and decision science to enhance safety and security in Singapore. Notable projects include:\n",
      "- **Process Modelling & Simulation**: This initiative focuses on utilizing data to create advanced solutions that improve operational efficiency and decision-making within the MHA. More details can be found on their [official page](https://www.htx.gov.sg/who-we-are/what-we-do/our-expertise/enterprise-group/xdata).\n",
      "- **EMPACT**: An AI-enabled, cloud-based immersive training platform designed for military training and other applications, allowing for the creation and management of interactive training programs. Further information is available on their [website](https://htxlabs.com/).\n"
     ]
    }
   ],
   "source": [
    "# Get JD PDF text\n",
    "reader = PdfReader(\"../pdf/JD Data Scientist - PM&S.pdf\")\n",
    "all_text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "w = simpleFlow(timeout=60, verbose=False)\n",
    "result = await w.run(input=all_text)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0c4af",
   "metadata": {},
   "source": [
    "> *Web-search not company-specific. Use web-search to supplement JD summary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afc131",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facinating-things",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
